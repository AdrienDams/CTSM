#! /usr/bin/env python
#
# singlept
#
# Script to extract a single grid point from global datasets
#

#  Import libraries
from __future__ import print_function

import sys
import os
import string
import logging
import subprocess
import argparse

import numpy as np
import xarray as xr

from getpass import getuser
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter


def get_parser():                                                                                                                                                                                  
        """Get parser object for this script."""
        parser = ArgumentParser(description=__doc__,
                                formatter_class=ArgumentDefaultsHelpFormatter)
        #parser = ArgumentParser(description='Setting the flags for running ./singlept script.')
        ## Add information about if this is optional and default
        parser.add_argument('--lat',
                    help='Single point latitude.', 
                    action="store",
                    dest="plat",
                    required=False,
                    type = plat_type,
                    default=42.5)
        parser.add_argument('--lon',
                    help='Single point longitude.', 
                    action="store",
                    dest="plon",
                    required=False,
                    type = plon_type,
                    default= 287.8 )
        parser.add_argument('--domain',
                    help='Flag for creating CLM domain file at single point.', 
                    action="store",
                    dest="create_domain",
                    required = False,
                    type = bool, 
                    default = True)
        parser.add_argument('--surface',
                    help='Flag for creating surface data file at single point.', 
                    action="store",
                    dest="create_surfdata",
                    required = False,
                    type = bool, 
                    default = True)
        parser.add_argument('--landuse',
                    help='Flag for creating landuse data file at single point.', 
                    action="store",
                    dest="create_landuse",
                    required = False,
                    type = bool, 
                    default = True)
        parser.add_argument('--datm',
                    help='Flag for creating DATM atmospheric forcing data at single point.', 
                    action="store",
                    dest="create_datm",
                    required = False,
                    type = bool, 
                    default = False)
        parser.add_argument('--datm_syr',
                    help='Start year for creating DATM atmospheric forcing at single point.', 
                    action="store",
                    dest="datm_syr",
                    required = False,
                    type = int, 
                    default = 1901)
        parser.add_argument('--datm_eyr',
                    help='End year for creating DATM atmospheric forcing at single point.', 
                    action="store",
                    dest="datm_eyr",
                    required = False,
                    type = int,
                    default = 2014)
        parser.add_argument('--nocrop', 
                    help='Create datasets without the extensive list of prognostic crop types', 
                    action="store_false", 
                    dest="crop_flag", 
                    default=True)
        return parser

def plat_type(x):
    x = float(x)
    if (x < -90) or (x > 90):
        raise argparse.ArgumentTypeError("ERROR: Latitude of single point should be between -90 and 90.")
    return x


def plon_type(x):
    x = float(x)
    if (x < 0) or (x > 360):
        raise argparse.ArgumentTypeError("ERROR: Latitude of single point should be between 0 and 360.")
    return x


def setup_logging(log_path, log_level):
    """
    Setup logging to log to console and log file.

    #  Log Levels
    # ------------
    # CRITICAL  50
    # ERROR     40
    # WARNING   30
    # INFO      20
    # DEBUG     10
    # NOTSET     0
    """

    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)

    # setup log file
    one_MB = 1000000
    handler = logging.handlers.RotatingFileHandler(
        log_path, maxBytes=(one_MB * 20), backupCount=5)

    fmt = logging.Formatter(
            '%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
            datefmt='%y-%m-%d %H:%M:%S')

    handler.setFormatter(fmt)
    root_logger.addHandler(handler)

    # setup logging to console
    strm_hndlr = logging.StreamHandler(sys.stdout)
    strm_hndlr.setFormatter(fmt)
    root_logger.addHandler(strm_hndlr)

    # redirect stdout/err to log file
    StreamToLogger.setup_stdout()
    StreamToLogger.setup_stderr()



class StreamToLogger(object):
    """
    Custom class to log all stdout and stderr streams.
    """
    def __init__(self, stream, logger, log_level=logging.INFO,
                 also_log_to_stream=False):
        self.logger = logger
        self.stream = stream
        self.log_level = log_level
        self.linebuf = ''
        self.also_log_to_stream = also_log_to_stream

    @classmethod
    def setup_stdout(cls, also_log_to_stream=True):
        """
        Setup logger for stdout
        """
        stdout_logger = logging.getLogger('STDOUT')
        sl = StreamToLogger(
            sys.stdout, stdout_logger, logging.INFO, also_log_to_stream
        )
        sys.stdout = sl

    @classmethod
    def setup_stderr(cls, also_log_to_stream=True):
        """
        Setup logger for stdout
        """
        stderr_logger = logging.getLogger('STDERR')
        sl = StreamToLogger(
            sys.stderr, stderr_logger, logging.ERROR, also_log_to_stream
        )
        sys.stderr = sl

    def write(self, buf):
        temp_linebuf = self.linebuf + buf 
        self.linebuf = ''
        for line in temp_linebuf.splitlines(True):
            # From the io.TextIOWrapper docs:
            #   On output, if newline is None, any '\n' characters written
            #   are translated to the system default line separator.
            # By default sys.stdout.write() expects '\n' newlines and then
            # translates them so this is still cross platform.
            if line[-1] == '\n':
                self.logger.log(self.log_level, line.rstrip())
            else:
                self.linebuf += line

    def flush(self):
        if self.linebuf != '': 
            self.logger.log(self.log_level, self.linebuf.rstrip())                                                
        self.linebuf = ''


#def SinglePointCase (self, plat, plon):
#    """
#    A case to encapsulate single point cases.
#    """
#    def __init__(self,plat, plon):
#        self.plat = plat
#        self.plon = plon 


def error( desc ):
     "error function"
     print( "ERROR:: "+desc )
     os.abort()


def AddTagToFilename(filename, tag):
    "Add a tag to a filename"
    # Expects file to end with [._]cYYMMDD.nc or [._]YYMMDD.nc
    # Add the tag to just before that ending part
    basename = os.path.basename(filename)
    cend = -10
    if ( basename[cend] == "c" ):
       cend = cend - 1
    if ( (basename[cend] != ".") and (basename[cend] != "_") ):
       error( "Trouble figuring out where to add tag to filename:"+filename )
    return( basename[:cend]+"_"+tag+basename[cend:] )
        

def create_domain_at_point (plat, plon,fdomain , fdomain2 ):
        print( "Open: "+fdomain )
        f1  = xr.open_dataset(fdomain)
        # create 1d coordinate variables to enable sel() method
        lon0=np.asarray(f1['xc'][0,:])
        lat0=np.asarray(f1['yc'][:,0])
        lon=xr.DataArray(lon0,name='lon',dims='ni',coords={'ni':lon0})
        lat=xr.DataArray(lat0,name='lat',dims='nj',coords={'nj':lat0})
        # assign() not working on cheyenne
        #f2=f1.assign({'lon':lon,'lat':lat})
        #f2=f1.assign()
        f2 = f1.copy()
        f2['lon'] = lon 
        f2['lat'] = lat 
        f2.reset_coords(['xc','yc'])
        # extract gridcell closest to plon/plat
        f3 = f2.sel(ni=plon,nj=plat,method='nearest')
        # expand dimensions
        f3 = f3.expand_dims(['nj','ni'])
 
        wfile=fdomain2
        # mode 'w' overwrites file
        f3.to_netcdf(path=wfile, mode='w')
        print('Successfully created file (fdomain2)'+fdomain2)
        f1.close(); f2.close(); f3.close()


def create_landuse_at_point (plat, plon,fluse , fluse2 ):
        print( "Open: "+fluse )
        f1  = xr.open_dataset(fluse)
        print (f1)
        # create 1d variables
        lon0=np.asarray(f1['LONGXY'][0,:])
        lon=xr.DataArray(lon0,name='lon',dims='lsmlon',coords={'lsmlon':lon0})
        lat0=np.asarray(f1['LATIXY'][:,0])
        lat=xr.DataArray(lat0,name='lat',dims='lsmlat',coords={'lsmlat':lat0})
        #f2=f1.assign({'lon':lon,'lat':lat})
        f2=f1.assign()
        f2['lon'] = lon 
        f2['lat'] = lat 
        print ("lat : ", lat)
        print ("lon : ", lon)  
        # extract gridcell closest to plon/plat
        f3 = f2.sel(lsmlon=plon,lsmlat=plat,method='nearest')
 
        # expand dimensions
        f3 = f3.expand_dims(['lsmlat','lsmlon'])
        # specify dimension order 
        #f3 = f3.transpose('time','lat','lon')
        f3 = f3.transpose(u'time', u'cft', u'natpft', u'lsmlat', u'lsmlon')
        #f3['YEAR'] = f3['YEAR'].squeeze()
 
        # revert expand dimensions of YEAR
        year = np.squeeze(np.asarray(f3['YEAR']))
        print (year)
        x = xr.DataArray(year, coords={'time':f3['time']}, dims='time', name='YEAR')
        x.attrs['units']='unitless'
        x.attrs['long_name']='Year of PFT data'
        f3['YEAR'] = x 
        #print(x)
        #print(f3)
        #stop
        # mode 'w' overwrites file
        f3.to_netcdf(path=fluse2, mode='w')
        print('created file (fluse2)'+fluse2)
        f1.close(); f2.close(); f3.close()


def create_datm_at_point (plat, plon, fdatmdomain, fdatmdomain2):
    print( "Open: "+fdatmdomain )
    f1  = xr.open_dataset(fdatmdomain)
    # create 1d coordinate variables to enable sel() method
    lon0=np.asarray(f1['xc'][0,:])
    lat0=np.asarray(f1['yc'][:,0])
    lon=xr.DataArray(lon0,name='lon',dims='ni',coords={'ni':lon0})
    lat=xr.DataArray(lat0,name='lat',dims='nj',coords={'nj':lat0})
          
    #f2=f1.assign({'lon':lon,'lat':lat})
    f2=f1.assign()
    f2['lon'] = lon
    f2['lat'] = lat
    f2.reset_coords(['xc','yc'])
    # extract gridcell closest to plon/plat
    f3 = f2.sel(ni=plon,nj=plat,method='nearest')
    # expand dimensions
    f3 = f3.expand_dims(['nj','ni'])
    wfile=fdatmdomain2
    # mode 'w' overwrites file
    f3.to_netcdf(path=wfile, mode='w')
    print('created file '+fdatmdomain2)
    f1.close(); f2.close(); f3.close()


def main ():




    '''
    #------------------------------------------------------------------#
    #---------------------  Instructions  -----------------------------#
    #------------------------------------------------------------------#
    load the following into your local environment
    module load python/2.7.14
    ncar_pylib

    After creating a case using a global compset, run preview_namelist.  
    From the resulting lnd_in file in the run directory, find the name 
    of the domain file, and the surface data file.  
    From the datm streams files (e.g. datm.streams.txt.CLMGSWP3v1.Precip)
    find the name of the datm forcing data domain file and forcing files.  
    Use these file names as the sources for the single point files to 
    be created (see below).

    After running this script, point to the new CLM domain and surface 
    dataset using the user_nl_clm file in the case directory.  In addition, 
    copy the datm.streams files to the case directory, with the prefix 
    'user_', e.g. user_datm.streams.txt.CLMGSWP3v1.Precip.  Change the 
    information in the user_datm.streams* files to point to the single 
    point datm data (domain and forcing files) created using this script.  

    The domain file is not set via user_nl_clm, but requires changing 
    LND_DOMAIN and ATM_DOMAIN (and their paths) in env_run.xml.  

    Using single point forcing data requires specifying the nearest 
    neighbor mapping algorithm for the datm streams (usually they are 
    the first three in the list) in user_nl_datm: mapalgo = 'nn','nn','nn', 
    ..., where the '...' can still be 'bilinear', etc, depending on the 
    other streams that are being used, e.g. aerosols, anomaly forcing, 
    bias correction.

    The file env_mach_pes.xml should be modified to specify a single 
    processor.  The mpi-serial libraries should also be used, and can be 
    set in env_build.xml by changing "MPILIB" to "mpi-serial" prior to 
    setting up the case.  

    The case for the single point simulation should have river routing 
    and land ice models turned off (i.e. the compset should use stub 
    models SROF and SGLC)

    to run the script
    ./singlept
    deactivate   #  to remove NPL from environment

    '''

    args = get_parser().parse_args()

    #  Set control flags

    #-- Setup by default to run for Harvard Forest

    #--  Specify point to extract
    plon = args.plon 
    plat = args.plat

    #--  Create regional CLM domain file
    create_domain   = args.create_domain
    #--  Create CLM surface data file
    create_surfdata = args.create_surfdata
    #--  Create CLM surface data file
    create_landuse  = args.create_landuse
    #--  Create single point DATM atmospheric forcing data
    create_datm     = args.create_datm
    datm_syr = args.datm_syr
    datm_eyr = args.datm_eyr

    crop_flag = args.crop_flag



    #--  Modify landunit structure
    overwrite_single_pft = True
    dominant_pft         = 7 #BETr
    zero_nonveg_landunits= True
    uniform_snowpack     = True
    no_saturation_excess = True


    log_file = "/glade/scratch/negins/ctsm_singlept/tools/contrib/log.file"
    # create dir if it does not exist
    log_dir = os.path.dirname(log_file)
    if not os.path.exists(log_dir):
        os.mkdir(log_dir)

    log_level =  logging.DEBUG
    setup_logging(log_file, log_level)

    print( "singlept script to extract out a single point from the global CTSM inputdata datasets." )
    myname=getuser()
    pwd=os.getcwd()
    print("User = "+myname)
    print("Current directory = "+pwd)








    if crop_flag:
        num_pft      = "78"
    else: 
        num_pft      = "16"


    logging.debug(' crop_flag = '+ crop_flag.__str__()+ ' num_pft ='+ num_pft)



    #--  Specify input and output directories
    dir_output='/glade/scratch/'+myname+'/single_point_negin/'
    if ( not os.path.isdir( dir_output ) ):
        os.mkdir( dir_output )
    dir_inputdata='/glade/p/cesmdata/cseg/inputdata/'
    dir_clm_forcedata='/glade/p/cgd/tss/CTSM_datm_forcing_data/'
    dir_input_datm=dir_clm_forcedata+'/atm_forcing.datm7.GSWP3.0.5d.v1.c170516/'
    dir_output_datm=dir_output + 'datmdata/'
    if ( not os.path.isdir( dir_output_datm ) ):
        os.mkdir( dir_output_datm )

    #--  Set input and output filenames
    tag=str(plon)+'_'+str(plat)

    #--  Set time stamp
    command='date "+%y%m%d"'
    x2=subprocess.Popen(command,stdout=subprocess.PIPE,shell='True')
    x=x2.communicate()
    timetag = x[0].strip()

    #--  Specify land domain file  ---------------------------------
    fdomain  = dir_inputdata+'share/domains/domain.lnd.fv0.9x1.25_gx1v7.151020.nc'
    fdomain2 = dir_output + AddTagToFilename( fdomain, tag )

    #--  Specify surface data file  --------------------------------
    fsurf    = dir_inputdata+'lnd/clm2/surfdata_map/release-clm5.0.18/surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_c190214.nc'

    fsurf2   = dir_output + AddTagToFilename( fsurf, tag )

    #--  Specify landuse file  -------------------------------------
    fluse    = dir_inputdata+'lnd/clm2/surfdata_map/release-clm5.0.18/landuse.timeseries_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr1850-2015_c190214.nc'
    fluse2   = dir_output + AddTagToFilename( fluse, tag )

    #--  Specify datm domain file  ---------------------------------
    fdatmdomain = dir_clm_forcedata+'atm_forcing.datm7.GSWP3.0.5d.v1.c170516/domain.lnd.360x720_gswp3.0v1.c170606.nc'
    fdatmdomain2  = dir_output_datm+AddTagToFilename( fdatmdomain, tag )

    #--  Create CTSM domain file
    if create_domain:
        create_domain_at_point (plat, plon,fdomain , fdomain2 )

    #--  Create CTSM surface data file
    if create_surfdata:
        print( "Open: "+fsurf )
        f1  = xr.open_dataset(fsurf)
        # create 1d variables
        lon0=np.asarray(f1['LONGXY'][0,:])
        lon=xr.DataArray(lon0,name='lon',dims='lsmlon',coords={'lsmlon':lon0})
        lat0=np.asarray(f1['LATIXY'][:,0])
        lat=xr.DataArray(lat0,name='lat',dims='lsmlat',coords={'lsmlat':lat0})
        #f2=f1.assign({'lon':lon,'lat':lat})
        f2=f1.assign()
        #f2 = f1.copy()
        f2['lon'] = lon
        f2['lat'] = lat
        # extract gridcell closest to plon/plat
        f3 = f2.sel(lsmlon=plon,lsmlat=plat,method='nearest')
        # expand dimensions
        f3 = f3.expand_dims(['lsmlat','lsmlon']).copy(deep=True)

        # modify surface data properties
        if overwrite_single_pft:
            f3['PCT_NAT_PFT'][:,:,:] = 0
            f3['PCT_NAT_PFT'][:,:,dominant_pft] = 100
        if zero_nonveg_landunits:
            f3['PCT_NATVEG'][:,:]  = 100
            f3['PCT_CROP'][:,:]    = 0
            f3['PCT_LAKE'][:,:]    = 0.
            f3['PCT_WETLAND'][:,:] = 0.
            f3['PCT_URBAN'][:,:,]   = 0.
            f3['PCT_GLACIER'][:,:] = 0.
        if uniform_snowpack:
            f3['STD_ELEV'][:,:] = 20.
        if no_saturation_excess:
            f3['FMAX'][:,:] = 0.

        # specify dimension order 
        #f3 = f3.transpose(u'time', u'cft', u'natpft', u'lsmlat', u'lsmlon')
        f3 = f3.transpose(u'time', u'cft', u'lsmpft', u'natpft', u'nglcec', u'nglcecp1', u'nlevsoi', u'nlevurb', u'numrad', u'numurbl', 'lsmlat', 'lsmlon')
        # mode 'w' overwrites file
        f3.to_netcdf(path=fsurf2, mode='w')
        print('created file (fsurf2)'+fsurf2)
        f1.close(); f2.close(); f3.close()

        ''' this is buggy; can't re-write a file within the same session
        # modify new surface data file
        if overwrite_single_pft:
            f1  = xr.open_dataset(fsurf2)
            f1['PCT_NAT_PFT'][:,:,:] = 0
            f1['PCT_NAT_PFT'][:,:,dominant_pft] = 100
            f1.to_netcdf(path='~/junk.nc', mode='w')
            #f1.to_netcdf(path=fsurf2, mode='w')
            f1.close()
        if zero_nonveg_landunits:
            #f1  = xr.open_dataset(fsurf2)
            f1  = xr.open_dataset('~/junk.nc')
            f1['PCT_NATVEG']  = 100
            f1['PCT_CROP']    = 0
            f1['PCT_LAKE']    = 0.
            f1['PCT_WETLAND'] = 0.
            f1['PCT_URBAN']   = 0.
            f1['PCT_GLACIER'] = 0.
            #f1.to_netcdf(path=fsurf2, mode='w')
            f1.to_netcdf(path='~/junk2.nc', mode='w')
            f1.close()
        '''
#--  Create CTSM transient landuse data file
    if create_landuse:
        create_landuse_at_point(plat, plon, fluse, fluse2)

    #--  Create single point atmospheric forcing data
    if create_datm:
        create_datm_at_point(plat, plon, fdatmdomain, fdatmdomain2)
        #--  specify subdirectory names and filename prefixes
        solrdir = 'Solar/'
        precdir = 'Precip/'
        tpqwldir = 'TPHWL/'
        prectag = 'clmforc.GSWP3.c2011.0.5x0.5.Prec.'
        solrtag = 'clmforc.GSWP3.c2011.0.5x0.5.Solr.'
        tpqwtag = 'clmforc.GSWP3.c2011.0.5x0.5.TPQWL.'

        #--  create data files  
        infile=[]
        outfile=[]
        for y in range(datm_syr,datm_eyr+1):
          ystr=str(y)
          for m in range(1,13):
             mstr=str(m) 
             if m < 10:
                mstr='0'+mstr

             dtag=ystr+'-'+mstr

             fsolar=dir_input_datm+solrdir+solrtag+dtag+'.nc'
             fsolar2=dir_output_datm+solrtag+tag+'.'+dtag+'.nc'
             fprecip=dir_input_datm+precdir+prectag+dtag+'.nc'
             fprecip2=dir_output_datm+prectag+tag+'.'+dtag+'.nc'
             ftpqw=dir_input_datm+tpqwldir+tpqwtag+dtag+'.nc'
             ftpqw2=dir_output_datm+tpqwtag+tag+'.'+dtag+'.nc'

             infile+=[fsolar,fprecip,ftpqw]
             outfile+=[fsolar2,fprecip2,ftpqw2]

        nm=len(infile)
        for n in range(nm):
            print(outfile[n])
            file_in = infile[n]
            file_out = outfile[n]
        
        
            f1  = xr.open_dataset(file_in)
            # create 1d coordinate variables to enable sel() method
            lon0=np.asarray(f1['LONGXY'][0,:])
            lat0=np.asarray(f1['LATIXY'][:,0])
            lon=xr.DataArray(lon0,name='lon',dims='lon',coords={'lon':lon0})
            lat=xr.DataArray(lat0,name='lat',dims='lat',coords={'lat':lat0})
            #f2=f1.assign({'lon':lon,'lat':lat})
            f2=f1.assign()
            f2['lon'] = lon
            f2['lat'] = lat
            f2.reset_coords(['LONGXY','LATIXY'])
            # extract gridcell closest to plon/plat
            f3  = f2.sel(lon=plon,lat=plat,method='nearest')
            # expand dimensions
            f3 = f3.expand_dims(['lat','lon'])
            # specify dimension order 
            f3 = f3.transpose(u'scalar','time','lat','lon')

            # mode 'w' overwrites file
            f3.to_netcdf(path=file_out, mode='w')
            f1.close(); f2.close(); f3.close()

          
        print('datm files written to: '+dir_output_datm)

    print( "Successfully ran script." )


if __name__ == "__main__":
    main()
